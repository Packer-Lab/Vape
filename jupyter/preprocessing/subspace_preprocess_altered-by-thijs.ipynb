{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-suite2P data preprocessing for communication subspace analysis\n",
    "\n",
    "To use the Semedo et al. code for reduced rank regression etc., the output should be an nxm matrix, where n=number of cells and m = (n trials x n samples in trial bin). If you save this as a .npy, it can be imported to MATLAB using the readNPY.m function found in [npy-matlab](https://github.com/kwikteam/npy-matlab) package.\n",
    "\n",
    "0.7 neuropil subtraction applied: this can be changed in utils_funcs.s2p_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ipython magic\n",
    "%matplotlib inline \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "import sys\n",
    "sys.path.append('/home/thijs/Google Drive/repos/Vape/utils')\n",
    "sys.path.append('/home/thijs/Google Drive/repos/Vape/jupyter/subspace')\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import utils_funcs as utils\n",
    "from paq2py import paq_read\n",
    "import sfuncs\n",
    "import pickle\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plane recorded at 11.5001 frames per second\n"
     ]
    }
   ],
   "source": [
    "# process 1 plane at a time for now\n",
    "plane = 1 # select plane to process. This is zero-indexed!\n",
    "tot_planes = 2 # how many planes did you record in total?\n",
    "fs = 1/0.043478004 # total frame rate Hz (1/frame period from xml file accompanying your recording. This will change when using custom ROIs)\n",
    "fs = fs / tot_planes #total frame rate is divided by number of planes\n",
    "print('plane recorded at {0:.4f} frames per second'.format(fs))\n",
    "\n",
    "cwd = os.getcwd()\n",
    "os.chdir('/home/thijs/Google Drive/oxford/packerlab/data_sarah')  # move to data folder\n",
    "\n",
    "raw = np.load('raw.npy')\n",
    "spks = np.load('spks.npy')\n",
    "\n",
    "## To load the stat and paq files, the np.load() functions needs to be altered:\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# call load_data with allow_pickle implicitly set to true\n",
    "stat = np.load('stat.npy')  # stat is a np array (of len n_neurons), each el is a dict\n",
    "paq = np.load('paq.npy')\n",
    "paq = paq.item()  # unpack to dictionary\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old\n",
    "\n",
    "dff = utils.dfof2(raw) #compute dF/F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split cells into S1 and S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving values for med key where condition met: original_index == 19\n",
      "X coordinate at S1S2 boundary = 458.0\n",
      "number S1 cells = 185 ; number S2 cells = 157\n"
     ]
    }
   ],
   "source": [
    "# In suite2p, find the cell that is furthest to the right in area S1 and note its index. \n",
    "# We will split the cells by a linear plane through the cell's X-coordinate\n",
    "c = sfuncs.getKeyFromStat_1plane(stat,'med',conditional=('original_index',19)) \n",
    "print('X coordinate at S1S2 boundary = {}'.format(c[0][1]))\n",
    "\n",
    "#split cells by boundary coordinate\n",
    "boundary = c[0][1]\n",
    "S1_indices, S2_indices = sfuncs.separateByX(stat, boundary) #split all cells in stat based on X coordinate\n",
    "print('number S1 cells =',len(S1_indices),'; number S2 cells =',len(S2_indices))\n",
    "\n",
    "#check the split has been performed correctly\n",
    "assert (len(S2_indices) + len (S1_indices) == stat.shape[0]) # should be True\n",
    "\n",
    "#check there are no cells in the same group (output should be empty)\n",
    "assert len(set(S1_indices) & set(S2_indices) ) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neurons in S1: 185, and S2: 157. Number of time points 12881\n"
     ]
    }
   ],
   "source": [
    "# Use indices to split data by region\n",
    "S1_stat = stat[S1_indices]\n",
    "S2_stat = stat[S2_indices]\n",
    "#get deconvolved spikes and dF/F for each area\n",
    "S1_spks = spks[S1_indices, :]\n",
    "S2_spks = spks[S2_indices, :]\n",
    "S1_dff = dff[S1_indices, :]\n",
    "S2_dff = dff[S2_indices, :]\n",
    "assert S1_dff.shape[1] == S2_dff.shape[1]\n",
    "print(f'Number of neurons in S1: {S1_dff.shape[0]}, and S2: {S2_dff.shape[0]}. Number of time points {S1_dff.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data by trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_frames = dff.shape[1] * tot_planes\n",
    "\n",
    "stim_start = utils.paq_data(paq, 'piezo_stim', threshold_ttl = True)\n",
    "stim_start_frames = sfuncs.stim_start_frame(paq,'piezo_stim')\n",
    "\n",
    "frame_clock = utils.paq_data(paq, 'frame_clock', threshold_ttl = True)\n",
    "frame_clock = frame_clock[:tot_frames] # get rid of foxy bonus frames\n",
    "frame_clock = frame_clock[plane::tot_planes] # just take clocks from the frame you care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the time window for each trial bin here. The authors of the Semedo paper take 1 second of data \n",
    "# per trial. As our stim is longer i'll try taking 2 seconds post-stimulus onset. \n",
    "# Worth playing with this\n",
    "\n",
    "pre_seconds = 0\n",
    "post_seconds = 2\n",
    "pre_frames = int(pre_seconds * fs) # number of frames before stim onset to inlcude in the trial\n",
    "post_frames = int(post_seconds * fs) # number of frames after stim onset to inlcude in the trial\n",
    "\n",
    "offset_s = 0 # option to offset trial bins from stimulus onset\n",
    "second_in_samples = 22000\n",
    "offset_frames = int(offset_s * second_in_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape spikes trial-based S1 (185, 23, 100)\n",
      "Shape spikes trial-based S2 (157, 23, 100)\n"
     ]
    }
   ],
   "source": [
    "#spikes S1\n",
    "spks_trials_S1 = utils.flu_splitter(S1_spks, frame_clock, stim_start, pre_frames, post_frames)\n",
    "spks_trials_S1 = spks_trials_S1[0]\n",
    "print(f'Shape spikes trial-based S1 {spks_trials_S1.shape}')\n",
    "\n",
    "#spikes S2\n",
    "spks_trials_S2 = utils.flu_splitter(S2_spks, frame_clock, stim_start, pre_frames, post_frames)\n",
    "spks_trials_S2 = spks_trials_S2[0]\n",
    "print(f'Shape spikes trial-based S2 {spks_trials_S2.shape}')\n",
    "\n",
    "#dFF S1\n",
    "dff_trials_S1 = utils.flu_splitter(S1_dff, frame_clock, stim_start+offset_frames, pre_frames, post_frames)\n",
    "dff_trials_S1 = dff_trials_S1[0]\n",
    "assert dff_trials_S1.shape == spks_trials_S1.shape\n",
    "\n",
    "#dFF S2\n",
    "dff_trials_S2 = utils.flu_splitter(S2_dff, frame_clock, stim_start+offset_frames, pre_frames, post_frames)\n",
    "dff_trials_S2 = dff_trials_S2[0]\n",
    "assert dff_trials_S2.shape == spks_trials_S2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtract mean and reshape back to 2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def zero_mean(data):\n",
    "    \"\"\"Assume 3D, last dimension is trials\"\"\"\n",
    "    assert len(data.shape) == 3\n",
    "    mean_data_reshaped = np.mean(data, 2)[:, :, np.newaxis]  # mean over trials\n",
    "    data_subbed = data - mean_data_reshaped  # zero mean data\n",
    "    assert data_subbed.shape == data.shape\n",
    "    data_subbed_rs = np.reshape(a=data_subbed, \n",
    "                                newshape=(data_subbed.shape[0], \n",
    "                                          data_subbed.shape[1] * data_subbed.shape[2]),\n",
    "                                order='F')  # reshape, take into account right order\n",
    "    return data_subbed_rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define empty dict\n",
    "dict_export = {'dff_s1': np.array([]), 'dff_s2': np.array([]), \n",
    "               'spikes_s1': np.array([]), 'spikes_s2': np.array([])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill dict with the zero-meaned data\n",
    "dict_export['dff_s1'] = zero_mean(dff_trials_S1)\n",
    "dict_export['dff_s2'] = zero_mean(dff_trials_S2)\n",
    "dict_export['spikes_s1'] = zero_mean(spks_trials_S1)\n",
    "dict_export['spikes_s2'] = zero_mean(spks_trials_S2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /home/thijs/Google Drive/oxford/packerlab/data_sarah\n"
     ]
    }
   ],
   "source": [
    "## Save as h5 file so it can be read in matlab/pyton\n",
    "print(f'Current directory: {os.getcwd()}')\n",
    "hfile = h5py.File('data_sarah_s1s2_zero_mean.h5', 'w') # create file\n",
    "for key, val in dict_export.items():  # store data\n",
    "    hfile.create_dataset(key, data=val)\n",
    "hfile.close()  # close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To load:\n",
    "\n",
    "# hfile = h5py.File('data_s1s2_zero_mean.h5', 'r')  # open with read access\n",
    "# data = hfile['dff_s1']  # memory-mapped access (until hfile is closed)\n",
    "# data = hfile['dff_s1'].value  # RAM access (permanent)\n",
    "# hfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
