{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "modified-obligation",
   "metadata": {},
   "source": [
    "# Notebook for construction of derivative dataframes\n",
    "## - uses basic dataframes (see ia_dataframe_basic.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "focused-complexity",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\n",
      "Suite2p path: ['/home/rlees/anaconda3/envs/suite2p/lib/python3.7/site-packages/suite2p']\n",
      "qnap_path: /home/rlees/mnt/qnap \n",
      "qnap_data_path /home/rlees/mnt/qnap/Data \n",
      "pkl_folder: /home/rlees/mnt/qnap/pkl_files \n",
      "master_path: /home/rlees/mnt/qnap/master_pkl/master_obj.pkl \n",
      "fig_save_path: /home/rlees/mnt/qnap/Analysis/Figures \n",
      "stam_save_path: /home/rlees/mnt/qnap/Analysis/STA_movies \n",
      "s2_borders_path: /home/rlees/mnt/qnap/Analysis/S2_borders\n"
     ]
    }
   ],
   "source": [
    "%run ./rob_setup_notebook.ipynb\n",
    "\n",
    "import utils.utils_funcs as uf\n",
    "import utils.gsheets_importer as gsi\n",
    "import utils.ia_funcs as ia \n",
    "from utils.paq2py import *\n",
    "\n",
    "session_type = 'interneuron'\n",
    "# 'sensory_nodetrend'\n",
    "# 'sensory_highactivity'\n",
    "# 'sensory_topactivity'\n",
    "# 'sensory_topcells'\n",
    "# 'sensory_extremefilter'\n",
    "# 'sensory_responsivecells'\n",
    "# 'sensory_2sec_test'\n",
    "# 'projection_nodetrend'\n",
    "# 'projection_2sec_test'\n",
    "# 'interneuron'\n",
    "\n",
    "projection = True if 'projection' in session_type else False\n",
    "interneuron = True if 'interneuron' in session_type else False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-dynamics",
   "metadata": {},
   "source": [
    "# Define dataframe paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "architectural-produce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "session_df_path = qnap_path + '/pkl_files/dataframes/' + session_type + '_session_df.pkl'\n",
    "experiment_df_path = qnap_path + '/pkl_files/dataframes/' + session_type + '_experiment_df.pkl'\n",
    "trial_df_path = qnap_path + '/pkl_files/dataframes/' + session_type + '_trial_df.pkl'\n",
    "cell_df_path = qnap_path + '/pkl_files/dataframes/' + session_type + '_cell_df.pkl'\n",
    "cell_trial_df_path = qnap_path + '/pkl_files/dataframes/' + session_type + '_cell_trial_df.pkl'\n",
    "cell_mean_timepoint_df_path = qnap_path + '/pkl_files/dataframes/' + session_type + '_cell_mean_timepoint_df.pkl'\n",
    "cell_trial_single_timepoint_df_path = (qnap_path + '/pkl_files/dataframes/' + session_type + \n",
    "                                       '_cell_trial_single_timepoint_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-freeze",
   "metadata": {},
   "source": [
    "# Load common dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "talented-needle",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "experiment_df = pd.read_pickle(experiment_df_path)\n",
    "cell_trial_df = pd.read_pickle(cell_trial_df_path)\n",
    "cell_df = pd.read_pickle(cell_df_path)\n",
    "cell_mean_timepoint_df = pd.read_pickle(cell_mean_timepoint_df_path)\n",
    "cell_trial_single_timepoint_df = pd.read_pickle(cell_trial_single_timepoint_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-library",
   "metadata": {},
   "source": [
    "# Find extreme responses from cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-webster",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Find and save extreme cells \n",
    "\n",
    "max_df = cell_trial_single_timepoint_df.groupby('cell_id').max()\n",
    "# max_df = cell_mean_timepoint_df.groupby('cell_id').max()\n",
    "\n",
    "# baseline_df = cell_mean_timepoint_df.query('timepoint < 0')\n",
    "# max_df = baseline_df.groupby('cell_id').max()\n",
    "\n",
    "threshold = 10\n",
    "\n",
    "if projection or interneuron:\n",
    "    data = max_df[(np.absolute(max_df['pr_resp']) > threshold) | \\\n",
    "                  (np.absolute(max_df['ps_resp']) > threshold) | \\\n",
    "                  (np.absolute(max_df['spont_resp']) > threshold)\n",
    "                 ]\n",
    "else:\n",
    "    data = max_df[(np.absolute(max_df['pr_resp']) > threshold) | \\\n",
    "                  (np.absolute(max_df['ps_resp']) > threshold) | \\\n",
    "                  (np.absolute(max_df['spont_resp']) > threshold) | \\\n",
    "                  (np.absolute(max_df['whisker_resp']) > threshold)\n",
    "                 ]\n",
    "\n",
    "extreme_cells = data.index.to_numpy()\n",
    "\n",
    "np.save(qnap_path + '/pkl_files/dataframes/' + session_type + '_extreme_cell_ids.npy', extreme_cells)\n",
    "\n",
    "extreme_cells = np.load(qnap_path + '/pkl_files/dataframes/' + session_type + '_extreme_cell_ids.npy', allow_pickle=True)\n",
    "\n",
    "# Plot extreme cells\n",
    "\n",
    "# df_ids = cell_mean_timepoint_df['cell_id'].isin(extreme_cells)\n",
    "# data = cell_mean_timepoint_df[df_ids]\n",
    "\n",
    "# for trial in ['pr', 'ps', 'spont', 'whisker']:\n",
    "#     sns.relplot(kind='line', y=trial + '_sta_resp', x='timepoint', col='cell_id', col_wrap=4, data=data, legend=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "blank-terrorist",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def removeExtremeCells(df, extreme_cells):\n",
    "    start_len = len(df)\n",
    "    \n",
    "    index = df.index.name\n",
    "    if index:    \n",
    "        df.reset_index(drop=False, inplace=True)\n",
    "    \n",
    "    extreme_cell_ids = df['cell_id'].isin(extreme_cells)\n",
    "    df = df[~extreme_cell_ids]\n",
    "    \n",
    "    if index:\n",
    "        df.set_index(index, inplace=True)\n",
    "    \n",
    "    end_len = len(df)\n",
    "    print(start_len - end_len, 'rows removed')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "approved-agreement",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "923 rows removed\n",
      "92300 rows removed\n",
      "167986 rows removed\n",
      "16798600 rows removed\n"
     ]
    }
   ],
   "source": [
    "removeExtremeCells(cell_df, extreme_cells).to_pickle(cell_df_path)\n",
    "removeExtremeCells(cell_trial_df, extreme_cells).to_pickle(cell_trial_df_path)\n",
    "removeExtremeCells(cell_mean_timepoint_df, extreme_cells).to_pickle(cell_mean_timepoint_df_path)\n",
    "removeExtremeCells(cell_trial_single_timepoint_df, extreme_cells).to_pickle(cell_trial_single_timepoint_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-validation",
   "metadata": {},
   "source": [
    "# Calculating influence of photostim for each cell (Chettih + Harvey style)\n",
    "\n",
    "1. Doing photostim response probability minus spont response probability is convoluted behind the calculation of responsivity on single trials, which is if the 500 ms average of post-stim response was >1 SD of the baseline, why not just use the raw dFF values to calculate influence?\n",
    "2. I was going to do photostim average response minus spont average response for each cell as an 'influence' value\n",
    "3. Chettih and Harvey do single trial photostim minus average of all spont trials, then normalise the difference by the standard deviation of all differences for that cell\n",
    "    - I think this accounts for the reliability of the influence for a single cell, i.e. cell with high standard deviation of differences will be penalised (lower influence) over one with small standard deviation\n",
    "    - Does it matter if the influence is reliable?\n",
    "    - Potentially normalise by the standard deviation of the baseline period to account for poor estimation of spiking activity, rather than the many bouts of delta Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-delight",
   "metadata": {},
   "source": [
    "## Photostim site influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "incredible-aside",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get the average spontaneous responses for all neurons from the cell_df\n",
    "\n",
    "cell_df = pd.read_pickle(cell_df_path)\n",
    "cell_trial_df = pd.read_pickle(cell_trial_df_path)\n",
    "\n",
    "cell_id_trial_df = cell_trial_df['cell_id'].reset_index(drop=True).to_numpy()\n",
    "cell_id_cell_df = cell_df.reset_index(drop=False)['cell_id'].to_numpy()\n",
    "\n",
    "sorted_ids = np.argsort(cell_id_cell_df)\n",
    "match_indices = np.searchsorted(cell_id_cell_df[sorted_ids], cell_id_trial_df)\n",
    "indices = sorted_ids[match_indices]\n",
    "\n",
    "avg_spont_response = cell_df['spont_sta_amp_resp'].iloc[indices].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "relevant-inclusion",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Append results to cell_df and calculate influence from them\n",
    "\n",
    "trials = ['pr', 'ps']\n",
    "\n",
    "for trial in ['pr','ps']:\n",
    "    \n",
    "    cell_trial_df['avg_spont_resp'] = avg_spont_response\n",
    "    cell_trial_df[trial + '_influence'] = cell_trial_df[trial + '_amp_resp'] - cell_trial_df['avg_spont_resp']\n",
    "\n",
    "    cell_trial_df[trial + '_influence_std'] = np.repeat(cell_trial_df.groupby('cell_id').std()[trial + '_influence'], 100).to_numpy()\n",
    "    cell_trial_df[trial + '_norm_influence'] = cell_trial_df[trial + '_influence']/cell_trial_df[trial + '_influence_std']\n",
    "    \n",
    "    cell_df[trial + '_avg_norm_influence'] = cell_trial_df.groupby('cell_id').mean()[trial + '_norm_influence']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-marble",
   "metadata": {},
   "source": [
    "## Control site influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "korean-oasis",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 99\r"
     ]
    }
   ],
   "source": [
    "# Leave-one-out for control trials and recalculate the average and difference of control trial from that average\n",
    "# Log that value as the influence on that trial\n",
    "\n",
    "# for each trial out of 100 x\n",
    "# take that trial (of interest) x\n",
    "# take the mean of the other 99 x\n",
    "# presumably now the same length and order x\n",
    "# subtract the mean of the 99 from the 1 x\n",
    "# that is the influence of that first control trial, save in array (2D, n x 100) where n = cells x\n",
    "# once full array, swap dims and reshape so that it goes 1-100 for each cell consecutively, not 1-ncells for each trial\n",
    "\n",
    "leftout_influence = np.full((100,len(cell_df)), np.nan)\n",
    "\n",
    "for i, trial in enumerate(np.arange(1, 101)):\n",
    "    trial_leftout = cell_trial_df.query('trial_num == ' + str(trial))\n",
    "    remaining_trials = cell_trial_df.query('trial_num != ' + str(trial))\n",
    "\n",
    "    remaining_mean_resp = remaining_trials.groupby('cell_id')['spont_amp_resp'].mean().to_numpy()\n",
    "    leftout_influence[i,:] = trial_leftout['spont_amp_resp'].to_numpy() - remaining_mean_resp\n",
    "    \n",
    "    print('trial', i, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "incorporated-notice",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cell_trial_df['spont_influence'] = np.swapaxes(leftout_influence, 0, 1).flatten()\n",
    "\n",
    "cell_trial_df['spont_influence_std'] = np.repeat(cell_trial_df.groupby('cell_id').std()['spont_influence'], 100).to_numpy()\n",
    "cell_trial_df['spont_norm_influence'] = cell_trial_df['spont_influence']/cell_trial_df['spont_influence_std']\n",
    "\n",
    "cell_df['spont_avg_norm_influence'] = cell_trial_df.groupby('cell_id').mean()['spont_norm_influence']\n",
    "\n",
    "cell_df.to_pickle(cell_df_path)\n",
    "cell_trial_df.to_pickle(cell_trial_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-iraqi",
   "metadata": {},
   "source": [
    "## No normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "unlikely-strip",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cell_df = pd.read_pickle(cell_df_path)\n",
    "cell_trial_df = pd.read_pickle(cell_trial_df_path)\n",
    "\n",
    "trials = ['pr', 'ps', 'spont']\n",
    "\n",
    "for trial in trials:\n",
    "\n",
    "    cell_df[trial + '_avg_norm_influence'] = cell_trial_df.groupby('cell_id').mean()[trial + '_influence']\n",
    "\n",
    "cell_df.to_pickle(cell_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-pizza",
   "metadata": {},
   "source": [
    "# Find manually annotated CTB targets (only S1 target region was annotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "interim-catering",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if projection:\n",
    "    \n",
    "    pr_experiments = experiment_df.index.str.contains('pr')\n",
    "    tiff_paths = experiment_df[pr_experiments].tiff_path\n",
    "    session_ids = experiment_df[pr_experiments].session_id\n",
    "\n",
    "    ctb_df = pd.DataFrame()\n",
    "\n",
    "    for session_id, tiff_path in zip(session_ids, tiff_paths):\n",
    "\n",
    "        ctb_annotation = ia.listdirFullpath(tiff_path, 'annotation') \n",
    "        ctb_annotation_img = tf.imread(ctb_annotation)\n",
    "\n",
    "        ctb_annotation_coords = np.where(ctb_annotation_img>0)\n",
    "        ctb_annotation_img[ctb_annotation_coords] = 1 # [y,x] coords\n",
    "\n",
    "        n_annotations = len(ctb_annotation_coords[0])\n",
    "\n",
    "        cell_img = np.zeros_like(ctb_annotation_img, dtype='uint16')\n",
    "\n",
    "        session_filter = cell_df.session_id.str.contains(session_id)\n",
    "        session_cell_df = cell_df[session_filter]\n",
    "\n",
    "        cell_id = session_cell_df.index\n",
    "        cell_x = session_cell_df.cell_x.to_numpy()\n",
    "        cell_y = session_cell_df.cell_y.to_numpy()\n",
    "\n",
    "        for i, coord in enumerate(zip(cell_y, cell_x)):\n",
    "            cell_img[coord] = i+1\n",
    "\n",
    "        # binary mask x cell image to get the cells that overlap with target areas\n",
    "        ctb_cells = cell_img*ctb_annotation_img\n",
    "\n",
    "        ctb_cells = np.unique(ctb_cells)[1:]-1 # correct the cell id due to zero indexing\n",
    "\n",
    "        ctb_id = np.zeros_like(cell_id.to_numpy(), dtype='bool')\n",
    "        ctb_id[ctb_cells] = True\n",
    "\n",
    "        temp_df = pd.DataFrame(data={'ctb_targets' : ctb_id},\n",
    "                               index=cell_id)\n",
    "\n",
    "        ctb_df = pd.concat([ctb_df, temp_df])\n",
    "\n",
    "    if 'ctb_targets' in cell_df.columns:\n",
    "        cell_df['ctb_targets'] = ctb_df\n",
    "    else:\n",
    "        cell_df.insert(9, 'ctb_targets', ctb_df, allow_duplicates=False)\n",
    "        \n",
    "    cell_df.to_pickle(cell_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-therapy",
   "metadata": {},
   "source": [
    "# Finding manually annotated interneurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "diverse-redhead",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if interneuron:\n",
    "    \n",
    "    pr_experiments = experiment_df.index.str.contains('pr')\n",
    "    tiff_paths = experiment_df[pr_experiments].tiff_path\n",
    "    session_ids = experiment_df[pr_experiments].session_id\n",
    "\n",
    "    int_df = pd.DataFrame()\n",
    "\n",
    "    for session_id, tiff_path in zip(session_ids, tiff_paths):\n",
    "\n",
    "        int_annotation = ia.listdirFullpath(tiff_path, 'interneurons.tif') \n",
    "        int_annotation_img = tf.imread(int_annotation)\n",
    "\n",
    "        int_annotation_coords = np.where(int_annotation_img>0)\n",
    "        int_annotation_img[int_annotation_coords] = 1 # [y,x] coords\n",
    "\n",
    "        n_annotations = len(int_annotation_coords[0])\n",
    "\n",
    "        cell_img = np.zeros_like(int_annotation_img, dtype='uint16')\n",
    "\n",
    "        session_filter = cell_df.session_id.str.contains(session_id)\n",
    "        session_cell_df = cell_df[session_filter]\n",
    "\n",
    "        cell_id = session_cell_df.index\n",
    "        cell_x = session_cell_df.cell_x.to_numpy()\n",
    "        cell_y = session_cell_df.cell_y.to_numpy()\n",
    "\n",
    "        for i, coord in enumerate(zip(cell_y, cell_x)):\n",
    "            cell_img[coord] = i+1\n",
    "\n",
    "        # binary mask x cell image to get the cells that overlap with target areas\n",
    "        int_cells = cell_img*int_annotation_img\n",
    "\n",
    "        int_cells = np.unique(int_cells)[1:]-1 # correct the cell id due to zero indexing\n",
    "\n",
    "        int_id = np.zeros_like(cell_id.to_numpy(), dtype='bool')\n",
    "        int_id[int_cells] = True\n",
    "\n",
    "        temp_df = pd.DataFrame(data={'int_cell' : int_id},\n",
    "                               index=cell_id)\n",
    "\n",
    "        int_df = pd.concat([int_df, temp_df])\n",
    "\n",
    "    if 'int_cell' in cell_df.columns:\n",
    "        cell_df['int_cell'] = int_df\n",
    "    else:\n",
    "        cell_df.insert(9, 'int_cell', int_df, allow_duplicates=False)\n",
    "        \n",
    "    cell_df.to_pickle(cell_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-bernard",
   "metadata": {},
   "source": [
    "# Calculate nearest neighbour distance to responsive target cell (or any target cell) for each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "current-carpet",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# DISTANCE TO RESPONSIVE TARGETS ONLY (STA_SIG)\n",
    "if interneuron:\n",
    "    dist_to_targ = pd.DataFrame(index=cell_df.index, columns=['dist_to_resp_targ'])\n",
    "\n",
    "    targets = ['pr']\n",
    "    trials = ['pr']\n",
    "\n",
    "    for target, trial in zip(targets, trials):\n",
    "        for name, group in cell_df.groupby('session_id'):\n",
    "            target_coords = group.query(target + '_target & sta_sig_' + trial)['cell_med'].to_numpy()\n",
    "            target_coords = np.vstack(target_coords)\n",
    "\n",
    "            target_query = group[target + '_target']\n",
    "            cell_meds = group['cell_med']\n",
    "            cell_ids = group.index\n",
    "\n",
    "            for i, (target_bool, cell_med, cell_id) in enumerate(zip(target_query, cell_meds, cell_ids)):\n",
    "\n",
    "                if target_bool == False:\n",
    "                    dists = spatial.distance.cdist([cell_med], target_coords)\n",
    "                    min_dist = np.amin(dists)\n",
    "\n",
    "                    dist_to_targ.loc[cell_id, 'dist_to_resp_targ'] = min_dist\n",
    "\n",
    "    cell_df = pd.concat([cell_df, dist_to_targ], axis=1) # maybe MERGE here instead?\n",
    "\n",
    "    cell_df.to_pickle(cell_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "involved-norwegian",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ANY TARGET (RESPONSIVE OR NOT)\n",
    "if interneuron:\n",
    "    dist_to_targ = pd.DataFrame(index=cell_df.index, columns=['dist_to_targ'])\n",
    "\n",
    "    targets = ['pr', 'pr']\n",
    "    trials = ['pr', 'spont']\n",
    "\n",
    "    for target, trial in zip(targets, trials):\n",
    "        for name, group in cell_df.groupby('session_id'):\n",
    "            target_coords = group.query(target + '_target')['cell_med'].to_numpy()\n",
    "            target_coords = np.vstack(target_coords)\n",
    "\n",
    "            target_query = group[target + '_target']\n",
    "            cell_meds = group['cell_med']\n",
    "            cell_ids = group.index\n",
    "\n",
    "            for i, (target_bool, cell_med, cell_id) in enumerate(zip(target_query, cell_meds, cell_ids)):\n",
    "\n",
    "                if target_bool == False:\n",
    "                    dists = spatial.distance.cdist([cell_med], target_coords)\n",
    "                    min_dist = np.amin(dists)\n",
    "\n",
    "                    dist_to_targ.loc[cell_id, 'dist_to_targ'] = min_dist\n",
    "\n",
    "    cell_df = pd.concat([cell_df, dist_to_targ], axis=1) # maybe MERGE here instead?\n",
    "\n",
    "    cell_df.to_pickle(cell_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-efficiency",
   "metadata": {},
   "source": [
    "# Constructing high target activity dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "obvious-interim",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Get trial indices where trial activity is low (for filtering out to get high trials only)\n",
    "\n",
    "cell_trial_df = pd.read_pickle(cell_trial_df_path)\n",
    "cell_df = pd.read_pickle(cell_df_path)\n",
    "\n",
    "def filter_trial_df(df1, df2, target_string, resp_string):\n",
    "    cell_ids = df1.query('@target_string').index\n",
    "    target_filter = df2['cell_id'].isin(cell_ids)\n",
    "    filtered_df = df2[target_filter]\n",
    "    \n",
    "    return df3.groupby(['session_id', 'trial_num']).sum()['@resp_string']\n",
    "    \n",
    "pr_target_trial_amp_resp = filter_trial_df(cell_df, cell_trial_df, 'pr_target', 'pr_amp_resp')\n",
    "ps_target_trial_amp_resp = filter_trial_df(cell_df, cell_trial_df, 'ps_target', 'ps_amp_resp')\n",
    "sham_target_trial_amp_resp = filter_trial_df(cell_df, cell_trial_df, 'pr_target', 'spont_amp_resp')\n",
    "\n",
    "sham_indices = sham_target_trial_amp_resp.index.sortlevel()\n",
    "pr_indices = pr_target_trial_amp_resp.index[pr_target_trial_amp_resp<15].sortlevel()\n",
    "ps_indices = ps_target_trial_amp_resp.index[ps_target_trial_amp_resp<15].sortlevel()\n",
    "\n",
    "# KEEP 25 LARGEST (out of 100)\n",
    "\n",
    "sham_indices = sham_target_trial_amp_resp.groupby('session_id').nsmallest(75, keep='first').index \\\n",
    "                .droplevel(level=0).sortlevel()\n",
    "pr_indices = pr_target_trial_amp_resp.groupby('session_id').nsmallest(75, keep='first').index \\\n",
    "                .droplevel(level=0).sortlevel()\n",
    "ps_indices = ps_target_trial_amp_resp.groupby('session_id').nsmallest(75, keep='first').index \\\n",
    "                .droplevel(level=0).sortlevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "computational-reserve",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CELL TRIAL SINGLE TIMEPOINT DF modifications\n",
    "\n",
    "cell_trial_single_timepoint_df = pd.read_pickle(cell_trial_single_timepoint_df_path)\n",
    "cell_trial_single_timepoint_df.set_index(['session_id', 'trial_num'], inplace=True)\n",
    "cell_trial_single_timepoint_df = cell_trial_single_timepoint_df.round({'timepoint' : 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-appliance",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CELL MEAN TIMEPOINT DF creation\n",
    "\n",
    "for i, (sham_mi, pr_mi, ps_mi) in enumerate(zip(sham_indices[0], pr_indices[0], ps_indices[0])):\n",
    "    cell_trial_single_timepoint_df.loc[sham_mi, 'spont_resp'] = np.nan\n",
    "    cell_trial_single_timepoint_df.loc[pr_mi, 'pr_resp'] = np.nan\n",
    "    cell_trial_single_timepoint_df.loc[ps_mi, 'ps_resp'] = np.nan\n",
    "    \n",
    "cell_mean_timepoint_df = cell_trial_single_timepoint_df.groupby(['cell_id', 'timepoint']).mean()\n",
    "\n",
    "cell_mean_timepoint_df.reset_index(inplace=True, drop=False)\n",
    "\n",
    "cell_mean_timepoint_df.rename(columns={'pr_resp' : 'pr_sta_resp',\n",
    "                                       'ps_resp' : 'ps_sta_resp',\n",
    "                                       'spont_resp' : 'spont_sta_resp',\n",
    "                                       'whisker_resp' : 'whisker_sta_resp'\n",
    "                                      }, inplace=True)\n",
    "\n",
    "cell_mean_timepoint_df['frame'] = np.tile(range(0,182), len(cell_mean_timepoint_df.cell_id.unique()))\n",
    "\n",
    "cell_mean_timepoint_df['session_id'] = cell_mean_timepoint_df['cell_id'].str[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "falling-christianity",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CELL DF\n",
    "\n",
    "cell_df_copy = cell_mean_timepoint_df.query('(timepoint > 0.35) & (timepoint < 0.85)') \\\n",
    "                                     .groupby('cell_id') \\\n",
    "                                     .mean() \\\n",
    "                                     .rename(columns={'pr_sta_resp' : 'pr_sta_amp_resp',\n",
    "                                                      'ps_sta_resp' : 'ps_sta_amp_resp',\n",
    "                                                      'spont_sta_resp' : 'spont_sta_amp_resp',\n",
    "                                                      'whisker_sta_resp' : 'whisker_sta_amp_resp'\n",
    "                                                     }) \\\n",
    "                                     .iloc[:,-5:]\n",
    "\n",
    "cell_df['pr_sta_amp_resp'] = cell_df_copy['pr_sta_amp_resp']\n",
    "cell_df['ps_sta_amp_resp'] = cell_df_copy['ps_sta_amp_resp']\n",
    "cell_df['spont_sta_amp_resp'] = cell_df_copy['spont_sta_amp_resp']\n",
    "cell_df['whisker_sta_amp_resp'] = cell_df_copy['whisker_sta_amp_resp']\n",
    "\n",
    "cell_df['pr_resp_sign'] = np.sign(cell_df['pr_sta_amp_resp']) == 1\n",
    "cell_df['ps_resp_sign'] = np.sign(cell_df['ps_sta_amp_resp']) == 1\n",
    "cell_df['spont_resp_sign'] = np.sign(cell_df['spont_sta_amp_resp']) == 1\n",
    "cell_df['whisker_resp_sign'] = np.sign(cell_df['whisker_sta_amp_resp']) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "unlike-amazon",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CELL TRIAL DF\n",
    "\n",
    "baseline = cell_trial_single_timepoint_df.query('(timepoint < 0) & (timepoint > -0.5)').groupby(['cell_id', 'trial_num']).mean()\n",
    "post_stim = cell_trial_single_timepoint_df.query('(timepoint > 0.35) & (timepoint < 0.85)').groupby(['cell_id', 'trial_num']).mean()\n",
    "\n",
    "amp_resp = post_stim.loc[:, ['pr_resp', 'ps_resp', 'spont_resp', 'whisker_resp']] - baseline.loc[:, ['pr_resp', 'ps_resp', 'spont_resp', 'whisker_resp']]\n",
    "\n",
    "amp_resp.rename(columns = {'pr_resp' : 'pr_amp_resp',\n",
    "                           'ps_resp' : 'ps_amp_resp',\n",
    "                           'spont_resp' : 'spont_amp_resp',\n",
    "                           'whisker_resp' : 'whisker_amp_resp'\n",
    "                          }, inplace=True)\n",
    "\n",
    "cell_trial_df.reset_index(drop=False, inplace=True)\n",
    "cell_trial_df.set_index(['cell_id', 'trial_num'], inplace=True)\n",
    "\n",
    "cell_trial_df['pr_amp_resp'] = amp_resp['pr_amp_resp']\n",
    "cell_trial_df['ps_amp_resp'] = amp_resp['ps_amp_resp']\n",
    "cell_trial_df['spont_amp_resp'] = amp_resp['spont_amp_resp']\n",
    "cell_trial_df['whisker_amp_resp'] = amp_resp['whisker_amp_resp']\n",
    "\n",
    "cell_trial_df.reset_index(drop=False, inplace=True)\n",
    "cell_trial_df.set_index('cell_trial_id', inplace=True)\n",
    "cell_trial_df = cell_trial_df.iloc[:, :8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "later-madagascar",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SAVING DATAFRAMES (should reset index on some of them?)\n",
    "\n",
    "session_type = 'sensory_topactivity'\n",
    "\n",
    "df_list = [cell_df, cell_trial_df, cell_mean_timepoint_df, cell_trial_single_timepoint_df]\n",
    "df_names = ['_cell_df', '_cell_trial_df', '_cell_mean_timepoint_df', '_cell_trial_single_timepoint_df']\n",
    "\n",
    "for df, name in zip(df_list, df_names):\n",
    "    df_name = session_type + name\n",
    "\n",
    "    # Pickle the object output to save it for analysis\n",
    "    pkl_path = os.path.join(pkl_folder, 'dataframes', df_name + '.pkl')\n",
    "    df.to_pickle(pkl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-voltage",
   "metadata": {},
   "source": [
    "# Constructing 'responsive cells' dataframes (based on whisker response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "unusual-renaissance",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_df(df, indices):\n",
    "    df_filter = df.cell_id.isin(indices)\n",
    "    return df[df_filter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "trained-management",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "w_resp_cells = cell_df.query('sta_sig_whisker | pr_target | ps_target').index\n",
    "\n",
    "cell_mean_timepoint_df = filter_df(cell_mean_timepoint_df, w_resp_cells)\n",
    "cell_trial_single_timepoint_df = filter_df(cell_trial_single_timepoint_df, w_resp_cells)\n",
    "cell_trial_df = filter_df(cell_trial_df, w_resp_cells)\n",
    "\n",
    "df_filter = cell_df.index.isin(w_resp_cells)\n",
    "cell_df = cell_df[df_filter]\n",
    "\n",
    "session_type = 'sensory_responsivecells'\n",
    "\n",
    "df_list = [cell_df, cell_trial_df, cell_mean_timepoint_df, cell_trial_single_timepoint_df]\n",
    "df_names = ['_cell_df', '_cell_trial_df', '_cell_mean_timepoint_df', '_cell_trial_single_timepoint_df']\n",
    "\n",
    "for df, name in zip(df_list, df_names):\n",
    "    df_name = session_type + name\n",
    "\n",
    "    # Pickle the object output to save it for analysis\n",
    "    pkl_path = os.path.join(pkl_folder, 'dataframes', df_name + '.pkl')\n",
    "    df.to_pickle(pkl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-bhutan",
   "metadata": {},
   "source": [
    "# Constructing top cell dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-healing",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cell_ids = []\n",
    "\n",
    "for group_name, group_df in cell_df.groupby('session_id'):\n",
    "    df_len = len(group_df)\n",
    "    half_df_len = round(df_len/2)\n",
    "    \n",
    "    new_cell_ids = group_df[:half_df_len].index.to_numpy()\n",
    "    \n",
    "    cell_ids.extend(new_cell_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-indian",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "session_type = 'sensory_topcells'\n",
    "\n",
    "df_list = [cell_df, cell_trial_df, cell_mean_timepoint_df, cell_trial_single_timepoint_df]\n",
    "df_names = ['_cell_df', '_cell_trial_df', '_cell_mean_timepoint_df', '_cell_trial_single_timepoint_df']\n",
    "\n",
    "for df, name in zip(df_list, df_names):\n",
    "    if name == '_cell_df':\n",
    "        df_filter = df.index.isin(cell_ids)  \n",
    "    else:\n",
    "        df_filter = df.cell_id.isin(cell_ids)\n",
    "        \n",
    "    new_df = df[df_filter]\n",
    "\n",
    "    df_name = session_type + name\n",
    "\n",
    "    # Pickle the object output to save it for analysis\n",
    "    pkl_path = os.path.join(pkl_folder, 'dataframes', df_name + '.pkl')\n",
    "    new_df.to_pickle(pkl_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
